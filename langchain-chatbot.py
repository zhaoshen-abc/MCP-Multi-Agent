"""
å¤šæœåŠ¡å™¨ MCP + LangChain Agent ç¤ºä¾‹
---------------------------------
1. è¯»å– .env ä¸­çš„ LLM_API_KEY / BASE_URL / MODEL
2. è¯»å– servers_config.json ä¸­çš„ MCP æœåŠ¡å™¨ä¿¡æ¯
3. å¯åŠ¨ MCP æœåŠ¡å™¨ï¼ˆæ”¯æŒå¤šä¸ªï¼‰
4. å°†æ‰€æœ‰å·¥å…·æ³¨å…¥ LangChain Agentï¼Œç”±å¤§æ¨¡å‹è‡ªåŠ¨é€‰æ‹©å¹¶è°ƒç”¨
"""

import asyncio
import json
import logging
import os
from typing import Any, Dict, List

from dotenv import load_dotenv
from langchain import hub
from langchain.agents import AgentExecutor, create_openai_tools_agent
from langchain.chat_models import init_chat_model
from langchain_mcp_adapters.client import MultiServerMCPClient
from langchain_mcp_adapters.tools import load_mcp_tools

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ç¯å¢ƒé…ç½®
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class Configuration:
    """è¯»å– .env ä¸ servers_config.json"""

    def __init__(self) -> None:
        load_dotenv()
        self.api_key: str = os.getenv("LLM_API_KEY") or ""
        self.base_url: str | None = os.getenv("BASE_URL")  # DeepSeek ç”¨ https://api.deepseek.com
        print(self.base_url)
        self.model: str = os.getenv("MODEL") or "deepseek-chat"
        if not self.api_key:
            raise ValueError("âŒ æœªæ‰¾åˆ° LLM_API_KEYï¼Œè¯·åœ¨ .env ä¸­é…ç½®")

    @staticmethod
    def load_servers(file_path: str = "servers_config.json") -> Dict[str, Any]:
        with open(file_path, "r", encoding="utf-8") as f:
            return json.load(f).get("mcpServers", {})

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ä¸»é€»è¾‘
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def run_chat_loop() -> None:
    """å¯åŠ¨ MCP-Agent èŠå¤©å¾ªç¯"""
    cfg = Configuration()
    os.environ["DEEPSEEK_API_KEY"] = os.getenv("LLM_API_KEY", "")
    if cfg.base_url:
        os.environ["DEEPSEEK_API_BASE"] = cfg.base_url
    servers_cfg = Configuration.load_servers()

    # æŠŠ key æ³¨å…¥ç¯å¢ƒï¼ŒLangChain-OpenAI / DeepSeek ä¼šè‡ªåŠ¨è¯»å–
    os.environ["OPENAI_API_KEY"] = cfg.api_key
    if cfg.base_url:  # å¯¹ DeepSeek ä¹‹ç±»çš„è‡ªå®šä¹‰åŸŸåå¾ˆæœ‰ç”¨
        os.environ["OPENAI_BASE_URL"] = cfg.base_url

    # 1ï¸âƒ£ è¿æ¥å¤šå° MCP æœåŠ¡å™¨
    mcp_client = MultiServerMCPClient(servers_cfg)

    tools = await mcp_client.get_tools()         # LangChain Tool å¯¹è±¡åˆ—è¡¨

    logging.info(f"âœ… å·²åŠ è½½ {len(tools)} ä¸ª MCP å·¥å…·ï¼š {[t.name for t in tools]}")

    # 2ï¸âƒ£ åˆå§‹åŒ–å¤§æ¨¡å‹ï¼ˆDeepSeek / OpenAI / ä»»æ„å…¼å®¹ OpenAI åè®®çš„æ¨¡å‹ï¼‰
    llm = init_chat_model(
        model=cfg.model,
        model_provider="deepseek" if "deepseek" in cfg.model else "openai",
    )

    # 3ï¸âƒ£ æ„é€  LangChain Agentï¼ˆç”¨é€šç”¨ promptï¼‰
    prompt = hub.pull("hwchase17/openai-tools-agent")
    agent = create_openai_tools_agent(llm, tools, prompt)
    agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)

    # 4ï¸âƒ£ CLI èŠå¤©
    print("\nğŸ¤– MCP Agent å·²å¯åŠ¨ï¼Œè¾“å…¥ 'quit' é€€å‡º")
    while True:
        user_input = input("\nä½ : ").strip()
        if user_input.lower() == "quit":
            break
        try:
            result = await agent_executor.ainvoke({"input": user_input})
            print(f"\nAI: {result['output']}")
        except Exception as exc:
            print(f"\nâš ï¸  å‡ºé”™: {exc}")

    # 5ï¸âƒ£ æ¸…ç†
    await mcp_client.cleanup()
    print("ğŸ§¹ èµ„æºå·²æ¸…ç†ï¼ŒBye!")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# å…¥å£
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
    asyncio.run(run_chat_loop())